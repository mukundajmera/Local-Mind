"""
Stress Tests: Model Swapping
=============================

GPU torture tests to verify VRAM stability and concurrent safety.

Attack Vectors Tested:
1. Rapid-fire model switching (race conditions)
2. Concurrent switch requests (deadlock prevention)
3. VRAM leak detection
4. Switch during active inference
"""

import pytest
import asyncio
import time
import gc
from typing import List, Dict, Any
from datetime import datetime

# Mark all tests in this module as stress tests
pytestmark = pytest.mark.stress


def get_gpu_memory_mb() -> float:
    """
    Get current GPU memory usage in MB.
    
    Returns:
        Memory usage in MB, or 0.0 if GPU not available
    """
    try:
        import torch
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / (1024 ** 2)
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            return torch.mps.current_allocated_memory() / (1024 ** 2)
    except ImportError:
        pass
    return 0.0


class TestModelSwapStability:
    """
    Red Team: GPU Torture Tests
    
    Goal: Prove that rapid model switching doesn't cause OOM or memory leaks.
    """
    
    @pytest.fixture
    def model_manager(self):
        """Get ModelManager instance."""
        from apps.backend.services.model_manager import ModelManager
        return ModelManager.get_instance()
    
    @pytest.mark.asyncio
    async def test_sequential_switches(self, model_manager):
        """
        Test 1: Sequential Model Switches
        
        Verify that normal sequential switching works correctly.
        """
        models = ["llama3.2:1b", "llama3.2:3b", "mistral:7b"]
        
        for model_name in models:
            try:
                # Note: This will fail if models aren't actually available
                # In a real test, we'd mock the LLM service
                result = await model_manager.switch_model(model_name)
                
                assert result["status"] == "success", f"Switch to {model_name} failed"
                assert result["current_model"] == model_name, f"Current model mismatch"
                
                # Verify model info
                info = model_manager.get_model_info()
                assert info["current_model"] == model_name, "Model info mismatch"
                
            except Exception as e:
                # If actual models aren't available, verify the error is expected
                assert "Model loading failed" in str(e) or "LLMService" in str(e), \
                    f"Unexpected error during switch: {e}"
    
    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_rapid_fire_switches(self, model_manager):
        """
        Test 2: Rapid-Fire Model Switching (Race Condition Test)
        
        Attack: Fire 10 model switch requests in < 1 second.
        Expected: Only one switch should succeed at a time (locking works).
        """
        models = ["llama3.2:1b", "llama3.2:3b"] * 5  # 10 switches
        
        start_time = time.time()
        tasks = []
        
        # Fire all switches concurrently
        for model_name in models:
            task = asyncio.create_task(model_manager.switch_model(model_name))
            tasks.append(task)
            await asyncio.sleep(0.05)  # 50ms between requests
        
        # Wait for all to complete
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        elapsed = time.time() - start_time
        
        # Verify timing
        assert elapsed < 10.0, f"Rapid switches took too long: {elapsed}s"
        
        # Count successes and failures
        successes = sum(1 for r in results if isinstance(r, dict) and r.get("status") == "success")
        failures = sum(1 for r in results if isinstance(r, Exception))
        
        print(f"\nðŸ”¥ Rapid Fire Results: {successes} successes, {failures} failures in {elapsed:.2f}s")
        
        # At least some should succeed (locking prevents all from failing)
        # But not all should succeed simultaneously (that would indicate no locking)
        assert successes > 0, "ðŸš¨ FAILURE: No switches succeeded (possible deadlock)"
    
    @pytest.mark.asyncio
    @pytest.mark.timeout(60)
    async def test_concurrent_switches(self, model_manager):
        """
        Test 3: Concurrent Switch Requests (Deadlock Prevention)
        
        Attack: Multiple async tasks trying to switch simultaneously.
        Expected: No deadlocks, all requests complete (success or fail).
        """
        async def attempt_switch(model_name: str, task_id: int) -> Dict[str, Any]:
            """Attempt a model switch and track timing."""
            start = time.time()
            try:
                result = await model_manager.switch_model(model_name)
                elapsed = time.time() - start
                return {
                    "task_id": task_id,
                    "status": "success",
                    "model": model_name,
                    "elapsed": elapsed
                }
            except Exception as e:
                elapsed = time.time() - start
                return {
                    "task_id": task_id,
                    "status": "failed",
                    "model": model_name,
                    "error": str(e),
                    "elapsed": elapsed
                }
        
        # Create 5 concurrent tasks
        tasks = [
            attempt_switch("llama3.2:1b", i) if i % 2 == 0 else attempt_switch("llama3.2:3b", i)
            for i in range(5)
        ]
        
        # Run all concurrently
        results = await asyncio.gather(*tasks)
        
        # Verify all completed (no hangs/deadlocks)
        assert len(results) == 5, "ðŸš¨ DEADLOCK: Not all tasks completed!"
        
        # Verify reasonable timing (no task took > 30s)
        for result in results:
            assert result["elapsed"] < 30.0, \
                f"ðŸš¨ HANG: Task {result['task_id']} took {result['elapsed']:.2f}s"
        
        print(f"\nâœ… Concurrent Safety: All {len(results)} tasks completed")
        for r in results:
            print(f"  Task {r['task_id']}: {r['status']} in {r['elapsed']:.2f}s")
    
    @pytest.mark.asyncio
    async def test_vram_cleanup(self, model_manager):
        """
        Test 4: VRAM Cleanup Verification
        
        Verify that VRAM is properly released after model unload.
        """
        # Get baseline memory
        baseline_memory = get_gpu_memory_mb()
        
        # Unload any existing model
        await model_manager.unload_model()
        
        # Force garbage collection
        gc.collect()
        
        # Wait a moment for cleanup
        await asyncio.sleep(0.5)
        
        # Get memory after cleanup
        after_cleanup = get_gpu_memory_mb()
        
        print(f"\nðŸ’¾ VRAM: Baseline={baseline_memory:.2f}MB, After Cleanup={after_cleanup:.2f}MB")
        
        # Memory should be similar (within 10% tolerance for fragmentation)
        if baseline_memory > 0:
            memory_diff = abs(after_cleanup - baseline_memory)
            tolerance = baseline_memory * 0.1
            
            assert memory_diff <= tolerance, \
                f"ðŸš¨ MEMORY LEAK: VRAM increased by {memory_diff:.2f}MB (tolerance: {tolerance:.2f}MB)"
        else:
            print("âš ï¸  GPU not available, skipping VRAM verification")
    
    @pytest.mark.asyncio
    async def test_memory_leak_detection(self, model_manager):
        """
        Test 5: Memory Leak Detection (Multiple Cycles)
        
        Perform multiple load/unload cycles and verify no memory accumulation.
        """
        memory_samples = []
        
        for i in range(3):
            # Unload
            await model_manager.unload_model()
            gc.collect()
            await asyncio.sleep(0.5)
            
            # Measure
            memory = get_gpu_memory_mb()
            memory_samples.append(memory)
            
            print(f"  Cycle {i+1}: {memory:.2f}MB")
        
        if memory_samples[0] > 0:
            # Verify memory doesn't grow significantly
            max_memory = max(memory_samples)
            min_memory = min(memory_samples)
            growth = max_memory - min_memory
            
            # Allow 5% growth for fragmentation
            tolerance = memory_samples[0] * 0.05
            
            assert growth <= tolerance, \
                f"ðŸš¨ MEMORY LEAK: VRAM grew by {growth:.2f}MB over 3 cycles (tolerance: {tolerance:.2f}MB)"
            
            print(f"âœ… No memory leak detected (growth: {growth:.2f}MB, tolerance: {tolerance:.2f}MB)")
        else:
            print("âš ï¸  GPU not available, skipping leak detection")
    
    @pytest.mark.asyncio
    async def test_switch_error_recovery(self, model_manager):
        """
        Test 6: Error Recovery
        
        Verify that failed switches don't leave the system in a bad state.
        """
        # Try to switch to a non-existent model
        try:
            await model_manager.switch_model("nonexistent-model-xyz")
            assert False, "Should have raised an error for non-existent model"
        except Exception as e:
            assert "Model loading failed" in str(e) or "failed" in str(e).lower(), \
                f"Unexpected error message: {e}"
        
        # Verify ModelManager is still functional
        info = model_manager.get_model_info()
        assert info is not None, "ðŸš¨ FAILURE: ModelManager broken after failed switch"
        
        # Verify we can still perform operations
        available = await model_manager.get_available_models()
        assert isinstance(available, list), "ðŸš¨ FAILURE: Cannot get available models after error"


class TestModelSwapMetrics:
    """
    Red Team: Performance and Metrics Validation
    
    Verify that model switching meets performance requirements.
    """
    
    @pytest.mark.asyncio
    async def test_switch_timing(self):
        """
        Test: Model Switch Timing
        
        Verify that model switches complete in reasonable time.
        """
        from apps.backend.services.model_manager import ModelManager
        
        manager = ModelManager.get_instance()
        
        # Measure switch time (will fail if model not available, but timing is still valid)
        start = time.time()
        try:
            await manager.switch_model("llama3.2:1b")
        except Exception:
            pass  # Expected if model not available
        elapsed = time.time() - start
        
        # Switch should complete in < 10 seconds (even if it fails)
        assert elapsed < 10.0, f"ðŸš¨ PERFORMANCE: Switch took {elapsed:.2f}s (max: 10s)"
    
    @pytest.mark.asyncio
    async def test_lock_acquisition_time(self):
        """
        Test: Lock Acquisition Time
        
        Verify that lock acquisition doesn't cause excessive delays.
        """
        from apps.backend.services.model_manager import ModelManager
        
        manager = ModelManager.get_instance()
        
        # Measure lock acquisition
        start = time.time()
        async with manager._operation_lock:
            elapsed = time.time() - start
            
            # Lock should be acquired immediately if not held
            assert elapsed < 0.1, f"ðŸš¨ PERFORMANCE: Lock acquisition took {elapsed:.2f}s"


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short", "-s"])
